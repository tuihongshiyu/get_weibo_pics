import requestsimport urllib.requestfrom bs4 import BeautifulSoupimport pymysqlimport typesimport timeimport reimport os# from .list import list_getfrom multiprocessing import Poolhost = 'localhost'user = 'root'password = 'Tuihongshiyu5'database = 'weibo_pics'PIC_PATH = '/Users/HirosueRyouko/Pictures/From Weibo/'list_path = '/Users/HirosueRyouko/Pictures/From Weibo/ID_LIST.txt'def show_picpath():    print('PIC_PATH: ', PIC_PATH)def set_picpath(PIC_PATH_new):    PIC_PATH = str(PIC_PATH_new)    show_picpath()def error_test(user_id, test_page):    main(user_id, test_page)def get_time():    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))def download_pic(url, name, user_id, path=PIC_PATH):    pic = requests.get(url)    if not os.path.exists(path + user_id):        os.mkdir(path + user_id)    if os.path.exists(path + user_id + '/' + name + '.jpg'):        return True    # print(path + user_id + '/' + name + '.jpg')    try:        f = open(path + user_id + '/' + name + '.jpg', 'wb')        f.write(pic.content)        f.close()        return True    except:        return Falsedef main(user_id=5416247360, test_page=None, page_begin=1, path=PIC_PATH):    user_id = str(user_id)    containerid = get_containerid(user_id)    page_no = page_begin    Downloaded_num = 0    headers = {        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'    }    if not test_page == None:  # FOR TEST        print('-------Test Begin')        page_no = test_page        url = 'http://m.weibo.cn/container/getIndex?type=uid&value=' + str(            user_id) + '&containerid=' + containerid + '&page=' + str(page_no)        # 所用api为移动端        page = requests.get(url, headers=headers)        soup = str(BeautifulSoup(page.text, 'lxml'))        soup_dic = str2dic(soup)        print(soup_dic['cards'])        if (soup_dic == 'Error'):            print('Error in Page :', page_no)            page_no = page_no + 1        elif soup_dic['cards'] == []:            # print('--------All things are done')            return        else:            pics_info, top_info = get_dic_info(soup_dic)            print(pics_info)        return    print('-------Downloading Bgein,ID : ' + user_id)    while (True):        url = 'http://m.weibo.cn/container/getIndex?type=uid&value=' + user_id + '&containerid=' + containerid + '&page=' + str(            page_no)        page = requests.get(url, headers=headers)        soup_dic = str2dic(str(BeautifulSoup(page.text, 'lxml')))        if (soup_dic == 'Error'):            print('Error in Page :', page_no)            page_no = page_no + 1        elif soup_dic['cards'] == []:            # print('-------'+user_id+'Done\n','-------Till : ',get_time())            return        else:            pics_info, top_info = get_dic_info(soup_dic)            pics_info['user_id'] = user_id            print('id : ', user_id, ' Page: ', pics_info['page'], ' Num of urls: ', len(pics_info['url']))            pic_url = pics_info['url']            id = pics_info['id']            url_downloaded = []            info2mysql(pics_info)            if not nextpage(user_id, pics_info['id']):                print('No next page')                break            page_no = page_no + 1            time.sleep(0.5)def main_download(user_id):    url,id=undownloaded2download(user_id)    for i in range(url):        if download_pic(url=url[i],name=id[i],user_id=user_id):            setpic_downloaded(user_id=user_id,id=id[i])        else:            Nonedef setpic_downloaded(user_id,id):    try:        db = pymysql.connect(host, user, password, database, charset='utf8')        cursor = db.cursor()        # print('CONNECTING SUCESSFULLY')    except:        print('CONNECTING ERROR')    try:        sql = "update weibo_pics.user" + user_id + " set downloaded = \'1\' where id =" + "\"" + id + "\";"        cursor.execute(sql)    except:        print("ERROR")        return False    db.commit()    db.close()    return Truedef undownloaded2download(user_id):    pics_path = PIC_PATH + user_id + '/'    url=[]    id=[]    url_id=[]    try:        db = pymysql.connect(host, user, password, database, charset='utf8')        cursor = db.cursor()        # print('CONNECTING SUCESSFULLY')    except:        print('CONNECTING ERROR')    try:        sql = "select url,id from weibo_pics.user" + user_id + " where downloaded = \'0\';"        cursor.execute(sql)        url_id=list(cursor.fetchall())    except:        print("ERROR")    db.commit()    db.close()    for data in url_id:        data=list(data)        url.append(data[0])        id.append(data[1])    return url,iddef downloaded2mysql(user_id):    pics_path = PIC_PATH + user_id + '/'    pic_ids = os.listdir(pics_path)    try:        db = pymysql.connect(host, user, password, database, charset='utf8')        cursor = db.cursor()        # print('CONNECTING SUCESSFULLY')    except:        print('CONNECTING ERROR')    for pic_id in pic_ids:        id = pic_id[0:-4]        try:            sql = "update weibo_pics.user" + user_id + " set downloaded = \'1\' where id =" + "\"" + id + "\";"            cursor.execute(sql)        except:            print("ERROR")    db.commit()    db.close()    # print(pics)def nextpage(user_id, ids):    id_latest = picid_latest(user_id)    for id in ids:        if (id == id_latest):            return False        else:            None    return Nonedef picid_latest(user_id):    user_id = 'user' + user_id    try:        db = pymysql.connect(host, user, password, database, charset='utf8')        cursor = db.cursor()        # print('CONNECTING SUCESSFULLY')    except:        print('CONNECTING ERROR')    try:        sql = "select id,createat from weibo_pics." + user_id + " order by createat desc limit 1;"        cursor.execute(sql)        data = cursor.fetchall()        return data[0][0]    except:        print('select error')def get_containerid(user_id='5416247360'):    url = 'http://m.weibo.cn/container/getIndex?type=uid&value=' + user_id    page = requests.get(url)    soup = str(BeautifulSoup(page.text, 'lxml'))    soup_str = soup[15:-18]    soup_dic = str2dic(soup_str)    return soup_dic['tabsInfo']['tabs'][1]['containerid']def str2dic(str):    str = str.replace("true", '1')    str = str.replace("false", '0')    str = str.replace("null", '0')    pat = re.compile(r'<([^<>]*)>')    str = pat.sub('', str)    try:        dic = eval(str)    except:        str = str.replace(",\"", '\n\"')        pat = re.compile(r'/"([^/"]*)"')        str = pat.sub('', str)        return 'Error'    return dicdef get_dic_info(dic):    time = ''    page_info = {}    top_info = {'is_top': 0, 'num_top': 0}    page_info['page'] = dic['cardlistInfo']['page']    page_info['url'] = {}    page_info['id'] = {}    page_info['createdat'] = {}    num = 0    num2 = 0    for i in range(len(dic['cards'])):        try:            card = dic['cards'][i]['mblog']            card_str = str(dic['cards'][i]['mblog'])            createdat = sinatime2format(card['created_at'])            page_info['url'][num] = (card['original_pic']).replace('\\', '')            page_info['id'][num] = card['id']            page_info['createdat'][num] = createdat            num = num + 1            # pics_urls=re.findall('\'url\': \'.*.jpg\'',card_str)            if not re.search('retweeted_status', card_str) and re.search('\'pics\'', card_str):                for j in range(len(card['pics'])):                    pic_url = card['pics'][j]['large']['url'].replace('\\', '')                    page_info['url'][num] = pic_url                    page_info['id'][num] = card['pics'][j]['pid']                    page_info['createdat'][num] = createdat                    num = num + 1        except:            None        try:            if i == 0 and card['isTop'] == 1:                top_info['is_top'] = 1                top_info['num_top'] = len(card['pics']) + 1        except:            None    return page_info, top_infodef is_num_by_except(str):    try:        int(str)        return True    except:        return Falsedef info2mysql(info):  # 将数据存储入本地的mysql,若出现编码错误，则忽略该条，同名文件会覆盖从前的数据库    user_id = 'user' + info['user_id']    Urls = info['url']    Id = info['id']    Createdat = info['createdat']    try:        db = pymysql.connect(host, user, password, database, charset='utf8')        # print('CONNECTING SUCESSFULLY')    except:        print('CONNECTING ERROR')    cursor = db.cursor()    try:        cursor.execute("show tables" + ';')        tables = cursor.fetchall()        if tables == {} or (user_id,) not in cursor.fetchall():            sql1 = """CREATE TABLE """            sql2 = """ (                    createat datetime,                    url char(100) NOT NULL,                    id char(50),                    downloaded bool,                    primary key (url)                    )character SET=utf8;"""            sql = sql1 + user_id + sql2            cursor.execute(sql)        else:            None    except:        None    for i in range(len(Urls)):        sql = "insert ignore into " + user_id + """        values("""        createat = datetime2mysql(Createdat[i])        sql = sql + createat + ',\"' + Urls[i] + '\",' + '\"' + str(Id[i]) + '\",' + '0' + ');'        try:            cursor.execute(sql)        except:            print(sql, '\nILLEGAL TEXT')            continue    db.commit()    db.close()def sinatime2format(sina_time):    if (sina_time[0:1] == '今天'):        time = (get_time('today')) + sina_time[-6:]    elif (sina_time[0:1] == '昨天'):        time = get_time('yesterday') + sina_time[-6:]    elif (len(sina_time) == 11):        time = (get_time())[0:5] + sina_time    else:        time = sina_time    return timedef datetime2mysql(time):    time = time.replace(':', '')    time = time.replace('-', '')    time = time.replace(' ', '')    if (len(time) != 14):        return time + '00'    else:        return timedef list_get():    if not os.path.exists(list_path):        print('No exist')        return None    else:        f_r = open(list_path, 'r')        list = f_r.readlines()        # print(list)        for i in range(len(list)):            list[i] = list[i][:-1]        f_r.close()        return listdef get_all():    list = list_get()    # print(list)    # print('Parent process %s.' % os.getpid())    p = Pool(len(list) + 1)    for i in range(len(list)):        p.apply_async(main, args=(list[i],))    print('Waiting for all subprocesses done...')    p.close()    p.join()    print('All subprocesses done.')# print(datetime2mysql('2014-14-14 14:14:14'))# main()# print(sinatime2format('02-20 16:16'))# get_containerid()# error_test()# get_all()# picid_latest('1712539910')# downloaded2mysql('1712539910')undownloaded2download('1712539910')