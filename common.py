import requestsimport urllib.requestfrom bs4 import BeautifulSoupimport typesimport timeimport reimport osfrom multiprocessing import Poolfrom get_weibo_pics.download import *from get_weibo_pics.sql import *from get_weibo_pics.text_processing import *import datetimePIC_PATH = '/Users/HirosueRyouko/Pictures/From Weibo/'list_path = '/Users/HirosueRyouko/Pictures/From Weibo/ID_LIST.txt'def main(user_id):    picsinfo2localmysql(user_id=user_id)    main_download(user_id=user_id)def picsinfo2localmysql(user_id='5416247360', page_begin=1, path=PIC_PATH):    containerid = get_containerid(user_id)    page_no = page_begin    sum_error = 0    bottom = int(info_bottom(user_id=user_id))    headers = {        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'    }    while (True):        url = 'http://m.weibo.cn/container/getIndex?type=uid&value=' + user_id + '&containerid=' + containerid + '&page=' + str(            page_no)        page = requests.get(url, headers=headers)        soup_dic = str2dic(str(BeautifulSoup(page.text, 'lxml')))        if soup_dic == 'Error' or soup_dic['cards'] == []:            sum_error = sum_error + 1            if (sum_error >= 7):                setinfo_downloaded(user_id)                print('Done in page ', str(page), ' of ', user_id)                return            page_no = page_no + 1        else:            sum_error = 0            pics_info, top_info = get_dic_info(soup_dic)            pics_info['user_id'] = user_id            # print('id : ', user_id, ' Page: ', pics_info['page'], ' Num of urls: ', len(pics_info['url']))            info2mysql(pics_info)            if nextpage(user_id, pics_info['createat']) == False:                if (bottom == 1):                    print('No next page for ', user_id)                    return                else:                    None            else:                print('So next the page of ' + str(page_no) + ' for ', user_id)                page_no = page_no + 1                time.sleep(0.5)def nextpage(user_id, createat):    creatat_latest = latest_pic_createat(user_id)    for i in range(len(createat)):        str_time_latest = creatat_latest.strftime('%Y-%m-%d %H:%M')        if str_time_latest > createat[i]:            return False        else:            None    return Truedef get_dic_info(dic):    time = ''    page_info = {}    top_info = {'is_top': 0, 'num_top': 0}    page_info['page'] = dic['cardlistInfo']['page']    page_info['url'] = {}    page_info['id'] = {}    page_info['createat'] = {}    num = 0    num2 = 0    for i in range(len(dic['cards'])):        try:            card = dic['cards'][i]['mblog']            card_str = str(dic['cards'][i]['mblog'])            createat = sinatime2format(card['created_at'])            page_info['url'][num] = (card['original_pic']).replace('\\', '')            page_info['id'][num] = card['id']            page_info['createat'][num] = createat            num = num + 1            # pics_urls=re.findall('\'url\': \'.*.jpg\'',card_str)            if not re.search('retweeted_status', card_str) and re.search('\'pics\'', card_str):                for j in range(len(card['pics'])):                    pic_url = card['pics'][j]['large']['url'].replace('\\', '')                    page_info['url'][num] = pic_url                    page_info['id'][num] = card['pics'][j]['pid']                    page_info['createat'][num] = createat                    num = num + 1        except:            None        try:            if i == 0 and card['isTop'] == 1:                top_info['is_top'] = 1                top_info['num_top'] = len(card['pics']) + 1        except:            None    # for i in range(len(page_info['createat'])):    #     page_info['createat'][i]=datetime2mysql(page_info['createat'][i])    return page_info, top_infodef check_all_downloaded():    user_list = users_info()    for user_id in user_list:        downloaded2mysql(user_id)def get_all():    user_list = users_info()    # print(list)    # print('Parent process %s.' % os.getpid())    p = Pool(len(user_list) + 1)    for i in range(len(user_list)):        p.apply_async(main, args=(user_list[i],))    print('Waiting for all subprocesses done...')    p.close()    p.join()    print('All subprocesses done.')def onebyone():    sum_done = 0    user_list = users_info()    for user_id in user_list:        main(user_id=str(user_id))        sum_done = sum_done + 1        print(sum_done, ' of ', len(user_list), ' done')